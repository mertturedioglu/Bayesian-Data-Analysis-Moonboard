{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9e0e48b",
   "metadata": {},
   "source": [
    "<p style=\"text-align:right\">Mert Türedioğlu</p>\n",
    "<p style=\"text-align:right\">1856335</p>\n",
    "<p style=\"text-align:center\">cogs516-literature-review</p>\n",
    "<p style=\"text-align:center\"><strong>probabilistic model for difficulty assessment of climbing routes</strong></p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* #####  Tai, C. H., Wu, A., & Hinojosa, R. (2020). Graph neural networks in classifying rock climbing difficulties. Technical report\n",
    "\n",
    "##### * Aim: \n",
    "Their main goal is to classify MoonBoard problems into difficulty categories. They hope to replace the heuristic-based classification of routes with their Machine Learning model. They aim to help climbers estimate their routes with the help of this model. \n",
    "\n",
    "##### * Methods\n",
    "They were inspired by the NLP domain in which Graph Convolutional Network architectures have proven success in text classification. Like words constitute sentences, they conceived holds form MoonBoard routes. There are 140 different holds on MoonBoard 2016 layout. A problem consists of specific holds. The difficulty of a problem results from this selection of particular holds. An image of an example MoonBoard problem is below:\n",
    "\n",
    "<img src=\"./images/an-easy-problem.png\" alt=\"an-easy-problem\" width=\"250\" height=\"300\">\n",
    "\n",
    "The green circles indicate the starting holds. The blue circles are intermediary ones. Lastly, the red circle designates the top(i.e. end) holds. You can use only these holds to claim an ascent of this specific problem..\n",
    "\n",
    "Like I did in this project, they have scraped the MoonBoard website to create their problems dataset. Unlike me, they have scraped both benchmark and non-benchmark problems. I only scraped the benchmark ones. The reason for this, the benchmark routes are reliable concerning their assigned difficulty grade. Benchmark routes are the ones that MoonBoard crew evaluates and confirms their grades. There is an imbalance in the dataset. I think they needed much more routes to train their machine learning model. I believe that I do not need that many problems to fit my model. There are far more easier routes than difficult ones in the dataset. They have used a sampling method to overcome this imbalance. They have sampled 2000 routes from each category. They split problems of each category into 80-20 divisions. In the end, they got 14,080 problems in the training set and 3,520 problems in the test set.     \n",
    "\n",
    "They have preprocessed their problems into both one-hot and multi-hot encoded representations. Recall that there are 140 holds in MoonBoard 2016 layout. In their encoding, a problem is a 140-dimensional vector in which holds are either present or absent(i.e. 0 or 1).\n",
    "\n",
    "Heterogenous Corpus Graph:\n",
    "For text classification, heterogenous corpus graphs are used. In text GCN's, there are two kinds of nodes, words and documents. An example figure is below:\n",
    "<img src=\"./images/heterogenous-corpus-graph.jpg\" alt=\"heterogenous-corpus-graph\" width=\"500\" height=\"600\">\n",
    "In their MoonBoard application of this technique, words are holds, and documents are problems. They expect that problems with similar holds have similar difficulty grades.\n",
    "\n",
    "##### * Results\n",
    "To make a comparison between different techniques, they have experimented with 17 different machine learning techniques. They grouped them under three categories: baselines, dense, and GCN. The best performing models are the Graph Convolutional Networks. With my limited knowledge of Machine Learning models, I think the reason behind this is their one-hot representation of MoonBoard problems. Not all the models are suitable for one-hot encoding. Anyway, the classes are difficulty grades: from V1 to V14. And experiment column are the techniques they used. The F1 table per class is below:\n",
    "<img src=\"./images/paper-1-f1-table.jpg\" alt=\"paper-1-f1-table\" width=\"600\" height=\"720\">\n",
    "They found the 4-step GCN model best performing model. They reported that the 4-step GCN performed better with restricted datasets as they expected. They claim that the optimal number of graph convolutions are domain-specific and four graph convolutions worked well in this problem. \n",
    "\n",
    "---\n",
    "\n",
    "* #####  Dobles, A., Sarmiento, J. C., & Satterthwaite, P. (2017). Machine learning methods for climbing route classification.\n",
    "\n",
    "##### * Aim: \n",
    "Like the paper above, their main goal is to classify MoonBoard problems into difficulty categories. They want to eliminate the subjective aspect of classifying problems' difficulties and make this classification task accurate and consistent. This work is published before the one above. \n",
    "\n",
    "##### * Methods\n",
    "They have scraped 13,871 total problems. There were fewer problems when they conducted this research. Again, the dataset is imbalanced. The dataset is skewed towers the easier side. They randomly divided the dataset into 11,095 training, 1,388 validation, and 1,388 test. Yet, the division is not fully randomized; they kept the distribution of grades in each set. In their research, they provided the below figures. In these figures, the first one represents easier routes, the middle one is the intermediate problems, and lastly, the harder routes. From these figures, they found to classify intermediate routes was harder than the other two groups since all the holds were more or less used with the same frequencies. In contrast, there are specific holds used in easier and harder routes.\n",
    "\n",
    "<img src=\"./images/paper-2-frequency-figures.jpg\" alt=\"paper-2-frequency-figures\" width=\"750\" height=\"900\">\n",
    "\n",
    "For CNN They represented MoonBoard as $$\\mathbb{x}^{(i)} \\in {0,1}^{18x11} $$\n",
    "For the Naive Bayes and softmax classifiers each MoonBoard problem turned into a vector and  used as feature space as follows:\n",
    "$$\\mathbb{x}^{(i)} \\in {0,1}^{n}$$\n",
    "\n",
    "They have experimented with three different classifiers: Naive Bayes classifier, softmax regression classifier, and convolutional neural network classifier. They also used SVM, but they did not report it because of its poor performance on the validation set.\n",
    "\n",
    "They have used a Naive Bayes classifier using the Bernoulli event model, and its input problem matrix is like the previous paper. The values are {0, 1} where 0 is absence and 1 is the presence of a hold in a problem. Yet, they thought that the assumption of the Naive Bayes classifier is violated since they believe that holds in a route are not independent of each other. . \n",
    "\n",
    "A softmax classifier were also used hold vectors of $$\\mathbb{(0, 1)}^{n}$$. They used Batch gradient descent algorithm. In the first implementation, the model had a tendency to towards to common labels when predicting the routes' difficulties. They added the penalizing weighting below to overcome this issue: \n",
    "<img src=\"./images/paper-2-penalizing.jpg\" alt=\"paper-2-penalizing\" width=\"300\" height=\"360\">\n",
    "Yet, it did not help. This time model started to overfit to less common categories.\n",
    "\n",
    "They treated this problem as an image problem. Apart from the baseline models above, they also used Convolutional Neural Network classifier to assess the difficulty grades of routes. Their diagram of implementation is below:\n",
    "\n",
    "<img src=\"./images/paper-2-cnn-diagram.jpg\" alt=\"paper-2-cnn-diagram\" width=\"500\" height=\"600\">\n",
    "\n",
    "Hypothesis function:\n",
    "<img src=\"./images/paper-2-hypothesis-function.jpg\" alt=\"paper-2-hypothesis-function\" width=\"400\" height=\"480\">\n",
    "\n",
    "Loss function:\n",
    "<img src=\"./images/paper-2-loss-function.jpg\" alt=\"paper-2-loss-function\" width=\"500\" height=\"600\">\n",
    "\n",
    "##### * Results\n",
    "\n",
    "They tested performance of their three classifiers against humans. There are three metrics: accuracy, MAE(mean absolute error) and KL-divergence(Kullback-Leibler divergence). The results on validation is below:\n",
    "<img src=\"./images/paper-2-validation-results.jpg\" alt=\"paper-2-validation-results\" width=\"500\" height=\"600\">\n",
    "Their performance is poor against human performance. However, they think that the reason behind this is that people did not grade the routes by just looking at the image of a route. People climb it, and it allows them to make accurate assessments. \n",
    "\n",
    "The table below shows the comparison of three classifiers with respect to actual categories:\n",
    "<img src=\"./images/paper-2-frequency-table.jpg\" alt=\"paper-2-frequency-table\" width=\"600\" height=\"660\">\n",
    "Note that both baseline models did not label any route with a grade 8A or harder. According to the authors, the reason for this is that both classifiers tend to predict common categories and the data is skewed towards the easier side. They also noted that more complex CNN was not possible since the dataset is relatively small. \n",
    "\n",
    "<br></br>\n",
    "\n",
    "<br></br>\n",
    "\n",
    "\n",
    "##### Discussion:\n",
    "\n",
    "I believe that a probabilistic model with a well-featured dataset will outperform the models of both papers. One treated the problem as an NLP problem and the other as an image problem. Both were far from representing the causal relations between holds when assessing the difficulty of a problem. The first paper was able to describe the relation between holds and problems to a certain extent. Their model reflected their assumption that similar problems have similar holds. Yet, it was not enough to have a model with high accuracy. \n",
    "I am aware that the subjective nature of assessing the difficulty grade of a route makes the problem difficult. Moreover, although the grading scale is discrete, to a certain extent, there may be differences between the routes in the same difficulty class. Some routes are closer to one grade harder class; the others may be closer to one grade easier class. Because of this, I believe that the performances of classifiers would significantly get better if we test them in +-1 classes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
